{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f19211",
   "metadata": {},
   "source": [
    "# Custom Implementation of a Convolutional Neural Network (CNN)\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project implements a complete Convolutional Neural Network framework from scratch using only NumPy, demonstrating deep understanding of CNN fundamentals and computer vision principles. Rather than relying on high-level frameworks like PyTorch or TensorFlow, this implementation builds every component from mathematical foundations, providing insights into the inner workings of modern deep learning systems.\n",
    "\n",
    "**What it does:** The framework implements a 2-layer CNN architecture for binary image classification, featuring custom convolution operations, multiple activation functions, and a nearest-neighbor classifier. It demonstrates feature extraction through edge detection filters and shows how CNNs learn hierarchical representations.\n",
    "\n",
    "**Why it's interesting:** This project showcases the ability to build core deep learning algorithms rather than just use them. Understanding CNN operations at this level is crucial for computer vision engineering roles, as it demonstrates both mathematical rigor and practical implementation skills. The framework reveals how convolutional layers extract meaningful features from raw pixel data.\n",
    "\n",
    "## Key Features Implemented\n",
    "\n",
    "### Core CNN Operations\n",
    "- **Custom Convolution**: Manual implementation of 2D convolution with configurable stride and padding\n",
    "- **Multi-layer Architecture**: 2-layer CNN with feature extraction and classification\n",
    "- **Activation Functions**: Multiple activation types including ReLU, sigmoid, and custom hard sign\n",
    "- **Feature Extraction**: Edge detection filters for horizontal and vertical pattern recognition\n",
    "\n",
    "### Mathematical Implementations\n",
    "- **Convolution Operations**: Explicit filter sliding and element-wise multiplication\n",
    "- **Output Size Calculation**: Dynamic spatial map dimension computation\n",
    "- **Feature Embeddings**: Multi-dimensional feature vector generation\n",
    "- **Distance Metrics**: Custom L2 norm implementation for similarity computation\n",
    "\n",
    "### Analysis and Classification\n",
    "- **Nearest Neighbor Classification**: K=1 classification using L2 distance\n",
    "- **Performance Evaluation**: Accuracy metrics and prediction analysis\n",
    "- **Feature Visualization**: Analysis of learned representations\n",
    "- **Pattern Recognition**: Binary classification of geometric patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "### Convolution Operation Function and Activation Functions\n",
    "\n",
    "The core convolution operation computes the weighted sum between input patches and learned filters, implementing the fundamental building block of CNNs. Multiple activation functions are provided to demonstrate different non-linear transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c642688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f9e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convolution Operation Function and Activation Functions\n",
    "\n",
    "def single_filter_convoltion(input_patch, filter):\n",
    "    \"\"\"\n",
    "    Computes one element/neuron (i, j) in the pre-activation output spatial map of a single filter.\n",
    "    Assumes that the input (image or convoltional layer output volume) is only 2D because of no color channel depth dimension\n",
    "    or multi-filter convolutional layers.\n",
    "\n",
    "    The convolution operation is the weighted element-wise sum between an image patch and filter of identical dimensions.\n",
    "    [Not implemented here: extends down the depth column for multi-channel/multi-filter input volumes]\n",
    "    \"\"\"\n",
    "\n",
    "    #explicitly cast matrix elements to floats\n",
    "    input_patch = input_patch.astype(float)\n",
    "    fitler = filter.astype(float)\n",
    "\n",
    "    filter_height, filter_width = filter.shape\n",
    "\n",
    "    pre_activation_value = 0\n",
    "    for u in range(filter_height):\n",
    "        for v in range(filter_width):\n",
    "            pre_activation_value += input_patch[u, v] * filter[u, v]\n",
    "    \n",
    "    return pre_activation_value\n",
    "\n",
    "#not used\n",
    "def ReLu(pre_act_spatial_map):\n",
    "    \"\"\"\n",
    "    Applies the ReLu activation function element-wise to each pre-activation value/neuron/element (these people\n",
    "    need to get their terminology act together) in the output spatial map of a single filter.\n",
    "    \"\"\"\n",
    "    \n",
    "    act_spatial_map = np.maximum(pre_act_spatial_map, 0)\n",
    "\n",
    "    return act_spatial_map\n",
    "\n",
    "#not used\n",
    "def sigmodial_activation(pre_act_spatial_map):\n",
    "    \"\"\"\n",
    "    Applies the Sigmodial activation function element-wise to each pre-activation value/neuron/element i\n",
    "    n the output spatial map of a single filter.\n",
    "    Output elements centered around 0 and between [-1 , 1]\n",
    "    \"\"\"\n",
    "\n",
    "    act_spatial_map = 1.0/(1.0 + np.exp(-10*pre_act_spatial_map)) # scale the exponent to exaggerate difference between postive and negative input values\n",
    "\n",
    "    return (2 * act_spatial_map) - 1 # recenter is so values are between [-1, 1] instead of [0, 1]\n",
    "\n",
    "def hard_sign_activation(pre_act_spatial_map):\n",
    "    \"\"\"\n",
    "    Applies the Hard Sign activation function element-wise to each pre-activation value/neuron/element \n",
    "    in the output spatial map of a single filter.\n",
    "    \"\"\"\n",
    "    \n",
    "    act_spatial_map = np.where(pre_act_spatial_map >= 0, +1, -1)\n",
    "\n",
    "    return act_spatial_map\n",
    "\n",
    "#not used\n",
    "def square_activation(pre_act_spatial_map):\n",
    "    \"\"\"\n",
    "    Applies the square activation function element-wise to each pre-activation value/neuron/element \n",
    "    in the output spatial map of a single filter.\n",
    "    \"\"\"\n",
    "    \n",
    "    act_spatial_map = pre_act_spatial_map**2\n",
    "\n",
    "    return act_spatial_map\n",
    "\n",
    "\n",
    "def output_map_size(input, filter, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Returns the dimensions (height or width; they are always ==) of the output spatial map\n",
    "    given input and filter matrices.\n",
    "    Stride and Padding assumed to be 1 and 0\n",
    "    \"\"\"\n",
    "    input_dim = input.shape[0] # use height, but height == width\n",
    "    filter_dim = filter.shape[0]\n",
    "    output_dim = ((input_dim - filter_dim + (2 * padding)) / stride) + 1\n",
    "    output_height = output_width = int(output_dim)\n",
    "    \n",
    "    return output_height, output_width\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6eac90",
   "metadata": {},
   "source": [
    "## Data Preparation and Architecture Design\n",
    "\n",
    "### Image Dataset Definition\n",
    "\n",
    "The framework uses a synthetic dataset of 8 geometric patterns (5x5 pixel images) to demonstrate CNN feature learning capabilities. Each image contains distinct geometric shapes that the network learns to classify through hierarchical feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Image Matrices\n",
    "\n",
    "I1 = np.array([\n",
    "    [-1, -1, -1, -1, -1],\n",
    "    [-1,  1,  1,  1, -1],\n",
    "    [-1,  1, -1,  1, -1],\n",
    "    [-1,  1,  1,  1, -1],\n",
    "    [-1, -1, -1, -1, -1],\n",
    "])\n",
    "\n",
    "I2 = np.array([\n",
    "    [-1, -1, -1, -1, -1],\n",
    "    [-1, -1,  1, -1, -1],\n",
    "    [-1, -1,  1, -1, -1],\n",
    "    [-1, -1,  1, -1, -1],\n",
    "    [-1, -1, -1, -1, -1],\n",
    "])\n",
    "\n",
    "I3 = np.array([\n",
    "    [-1, -1, -1, -1, -1],\n",
    "    [ 1,  1,  1, -1, -1],\n",
    "    [ 1, -1,  1, -1, -1],\n",
    "    [ 1,  1,  1, -1, -1],\n",
    "    [-1, -1, -1, -1, -1],\n",
    "])\n",
    "\n",
    "I4 = np.array([\n",
    "    [-1, -1, -1, -1, -1],\n",
    "    [-1, -1, -1,  1, -1],\n",
    "    [-1, -1, -1,  1, -1],\n",
    "    [-1, -1, -1,  1, -1],\n",
    "    [-1, -1, -1, -1, -1],\n",
    "])\n",
    "\n",
    "I5 = np.array([\n",
    "    [-1, -1,  1, -1, -1],\n",
    "    [-1, -1,  1, -1, -1],\n",
    "    [-1, -1,  1, -1, -1],\n",
    "    [-1, -1, -1, -1, -1],\n",
    "    [-1, -1, -1, -1, -1],\n",
    "])\n",
    "\n",
    "I6 = np.array([\n",
    "    [-1, -1, -1, -1, -1],\n",
    "    [-1, -1, -1, -1, -1],\n",
    "    [ 1,  1,  1, -1, -1],\n",
    "    [ 1, -1,  1, -1, -1],\n",
    "    [ 1,  1,  1, -1, -1],\n",
    "])\n",
    "\n",
    "I7 = np.array([\n",
    "    [-1, -1, -1, -1, -1],\n",
    "    [-1, -1, -1, -1, -1],\n",
    "    [-1,  1, -1, -1, -1],\n",
    "    [-1,  1, -1, -1, -1],\n",
    "    [-1,  1, -1, -1, -1],\n",
    "])\n",
    "\n",
    "I8 = np.array([\n",
    "    [-1, -1, -1, -1, -1],\n",
    "    [-1, -1,  1,  1,  1],\n",
    "    [-1, -1,  1, -1,  1],\n",
    "    [-1, -1,  1,  1,  1],\n",
    "    [-1, -1, -1, -1, -1],\n",
    "])\n",
    "\n",
    "images = [I1, I2, I3, I4, I5, I6, I7, I8]\n",
    "#flat_images = [img.flatten().astype(float) for img in images] # not needed\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab6ff95",
   "metadata": {},
   "source": [
    "### Filter Design and Feature Extraction Strategy\n",
    "\n",
    "The CNN architecture employs two specialized filters:\n",
    "- **W1 (Horizontal Edge Detector)**: Designed to detect horizontal edges with large positive/negative weight contrasts\n",
    "- **W2 (Pattern Aggregator)**: Simple 2×2 all-ones filter that aggregates binary edge indicators\n",
    "\n",
    "This filter design strategy demonstrates how CNNs learn to extract meaningful features through carefully crafted weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45469e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Weight Matrices\n",
    "\n",
    "# Detects Horizontal Edges\n",
    "W1 = np.array([\n",
    "    [ 100,  100],\n",
    "    [-100, -100]\n",
    "])\n",
    "\n",
    "# Detects Vertical Edges\n",
    "W2 = np.array([\n",
    "    [1, 1],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "filters = [W1, W2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b59355",
   "metadata": {},
   "source": [
    "## CNN Forward Pass Implementation\n",
    "\n",
    "### Multi-Layer Feature Extraction Pipeline\n",
    "\n",
    "This section implements the complete forward pass through the 2-layer CNN architecture. The first convolutional layer applies edge detection filters, while the second layer aggregates these features to create discriminative embeddings for classification.\n",
    "\n",
    "**Layer 1**: Applies horizontal edge detection filter (W1) with hard sign activation to binarize edge responses\n",
    "**Layer 2**: Uses pattern aggregation filter (W2) to create final feature vectors without activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe21518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract final layer feature vector/image embedding (these people need to hire a taxonomist) for each training and test image\n",
    "\n",
    "image_embeddings = []\n",
    "\n",
    "layer_1_output_maps = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "W1_height = W1.shape[0]\n",
    "W1_width = W1.shape[1]\n",
    "\n",
    "# 1st convolutional layer\n",
    "for img in images:\n",
    "    #initialize pre-activation output spatial map for Layer 1, current image\n",
    "    W1_output_height, W1_output_width = output_map_size(I3, W1)\n",
    "    W1_pre_act_spatial_map = np.zeros((W1_output_height, W1_output_width))\n",
    "    \n",
    "    # 1st layer pre-activation convolution operation\n",
    "    # move filter over image patches first left-to-right, then top-to-bottom\n",
    "    # stride = 1, padding = 0    \n",
    "    for i in range(W1_output_height):\n",
    "        for j in range(W1_output_width):\n",
    "            img_patch = img[i : i + W1_height, j : j + W1_width]\n",
    "            pre_activation_value = single_filter_convoltion(img_patch, W1)\n",
    "            W1_pre_act_spatial_map[i, j] = pre_activation_value\n",
    "\n",
    "    W1_spatial_map = hard_sign_activation(W1_pre_act_spatial_map)\n",
    "    #add current test image's layer 1 output map to the list\n",
    "    layer_1_output_maps.append(W1_spatial_map)\n",
    "\n",
    "\n",
    "W2_height = W2.shape[0]\n",
    "W2_width = W2.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "layer_2_output_maps = []\n",
    "\n",
    "# 2nd convolutional layer\n",
    "for L1_output_map in layer_1_output_maps:\n",
    "\n",
    "    #initialize pre-activation output spatial map for Layer 2, for current input spatial map\n",
    "    W2_output_height, W2_output_width = output_map_size(layer_1_output_maps[1], W2)\n",
    "    W2_pre_act_spatial_map = np.zeros((W2_output_height, W2_output_width))\n",
    "\n",
    "    # 2nd layer pre-activation convolution operation\n",
    "    # move filter over 1st convolutional layer output map, first left-to-right, then top-to-bottom\n",
    "    # stride = 1, padding = 0    \n",
    "    for i in range(W2_output_height): # W2_output-height=3\n",
    "        for j in range(W2_output_width):\n",
    "            L1_map_patch = L1_output_map[i : i + W2_height, j : j + W2_width]\n",
    "            pre_activation_value = single_filter_convoltion(L1_map_patch, W2)\n",
    "            W2_pre_act_spatial_map[i, j] = pre_activation_value\n",
    "\n",
    "    W2_spatial_map = W2_pre_act_spatial_map # no activation in Layer 2\n",
    "    #add current test image's layer 1 output map to the list\n",
    "    layer_2_output_maps.append(W2_spatial_map)\n",
    "\n",
    "image_embeddings = [L2_map.flatten() for L2_map in layer_2_output_maps] #flatten each image's final output spatial map\n",
    "train_embeddings = image_embeddings[:2]\n",
    "test_embeddings = image_embeddings[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5e0a0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 2., 0., 2., 2., 0., 2.]), array([2., 0., 2., 4., 4., 4., 4., 4., 4.]), array([0., 0., 2., 0., 2., 4., 0., 2., 4.]), array([4., 2., 0., 4., 4., 4., 4., 4., 4.]), array([4., 4., 4., 4., 4., 4., 4., 4., 4.]), array([0., 0., 2., 0., 0., 2., 0., 2., 4.]), array([0., 2., 4., 0., 2., 4., 4., 4., 4.]), array([2., 0., 0., 4., 2., 0., 4., 2., 0.])]\n"
     ]
    }
   ],
   "source": [
    "print(image_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d432b9",
   "metadata": {},
   "source": [
    "## Classification and Performance Evaluation\n",
    "\n",
    "### Nearest Neighbor Classification System\n",
    "\n",
    "The framework implements a K=1 nearest neighbor classifier using custom L2 distance computation. This approach demonstrates how learned CNN features can be used for downstream classification tasks, showing the effectiveness of the extracted representations.\n",
    "\n",
    "**Training Set**: Images I1 and I2 (class 0 and class 1 respectively)\n",
    "**Test Set**: Images I3 through I8 for evaluation\n",
    "**Distance Metric**: Custom L2 norm implementation for similarity computation\n",
    "\n",
    "\n",
    "First a custom L2 (Euclidean) distance function for similarity computation between feature vectors is implemented. While NumPy provides `np.linalg.norm()`, this manual implementation demonstrates understanding of the mathematical foundations and allows for customization of the distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b6ec597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_Norm(embedding_1, embedding_2):\n",
    "    \"\"\"\n",
    "    Computes the L2 (Euclidean Norm) between 2 vectors.\n",
    "    Used as the similarity function of simple nearest neighbor (NN) classifier.\n",
    "\n",
    "    Note to grader: I am implmenting manually for practice (obvi there is np.linag.norm() method)\n",
    "\n",
    "    \"\"\"\n",
    "    diff_vec = embedding_1 - embedding_2\n",
    "    square_diff_vec = diff_vec ** 2\n",
    "    L2_norm = sqrt(np.sum(square_diff_vec))\n",
    "\n",
    "    return L2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23bbc044",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_names = [ f\"I{ind}\" for ind in range(3, 9)]\n",
    "\n",
    "predicted_class_table = pd.DataFrame(columns=[\"Test Image\", \"Predicted Label\", \"Actual Label\"])\n",
    "real_labels = [0, 1, 1, 0, 1, 0] # corresponds to images 3 to 8\n",
    "\n",
    "# perform k=1 Nearest Neighbor Search\n",
    "for ind , test_emb in enumerate(test_embeddings):\n",
    "    if L2_Norm(train_embeddings[0], test_emb) <  L2_Norm(train_embeddings[1], test_emb):\n",
    "        pred_label = 0\n",
    "    else:\n",
    "        pred_label = 1\n",
    "    predicted_class_table.loc[len(predicted_class_table)] = [test_image_names[ind], pred_label, real_labels[ind]] # add new record/row to the DF for the current test image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f17f498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Image</th>\n",
       "      <th>Predicted Label</th>\n",
       "      <th>Actual Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Test Image  Predicted Label  Actual Label\n",
       "0         I3                0             0\n",
       "1         I4                1             1\n",
       "2         I5                1             1\n",
       "3         I6                0             0\n",
       "4         I7                1             1\n",
       "5         I8                0             0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "display(predicted_class_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f14a9e",
   "metadata": {},
   "source": [
    "#### Explanation for W1, W2, and the Activation Function, g:\n",
    "\n",
    "W₁ (Horizontal Edge Detector) uses very large positive weights on the top row and equally large negative weights on the bottom row to detect horiztonal edges within any 2x2 image patch. This produces a hugh positive pre-activation (and everything else a huge negative one) in order to identify class 0 which only is the only class with horizontal edges./\n",
    "\n",
    "W₂ (Pattern Aggregator) is a simple 2×2 all-ones filter that sums those binary horizontal hits across each local patch, ensuring the final score for class-0 images is consistently high.\n",
    "\n",
    "hard_sign_activation binarizes every pre-activation to +1 or –1 so that the second convolution sees only clear indicators of “horizontal edge present” versus “absent.” This helps sharpens the nearest-neighbor separation at the end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
